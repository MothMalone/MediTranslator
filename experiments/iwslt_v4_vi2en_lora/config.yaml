# IWSLT Version 4: Vi→En with LoRA Fine-tuning & Back-Translation
# Advanced configuration combining LoRA for efficient tuning and back-translation data

# Data Configuration
data:
  src_lang: "vi"
  tgt_lang: "en"
  
  # Primary training data
  train_src: "data/raw_IWSLT'15_en-vi/train.vi.txt"
  train_tgt: "data/raw_IWSLT'15_en-vi/train.en.txt"
  
  # Back-translation synthetic data (generated from En→Vi model)
  backtranslation_enabled: true
  backtranslation_src: "data/backtranslation/synthetic_train.vi.txt"  # Generated from En
  backtranslation_tgt: "data/backtranslation/synthetic_train.en.txt"  # Original En (from mono En data)
  backtranslation_ratio: 0.5  # Use 50% synthetic data mixed with original
  
  # Validation & Test
  val_src: "data/raw_IWSLT'15_en-vi/tst2012.vi.txt"
  val_tgt: "data/raw_IWSLT'15_en-vi/tst2012.en.txt"
  test_src: "data/raw_IWSLT'15_en-vi/tst2013.vi.txt"
  test_tgt: "data/raw_IWSLT'15_en-vi/tst2013.en.txt"
  
  max_seq_length: 128
  # Data filtering
  filter_by_length: true
  min_length: 3
  max_length_ratio: 1.5  # Filter pairs where len(src)/len(tgt) > 1.5

# Vocabulary - Optimized for medical domain
vocab:
  tokenization: "bpe"
  src_vocab_size: 16000
  tgt_vocab_size: 16000
  min_freq: 2

# Model - Larger & More Capable
model:
  d_model: 768      # Increased from 512
  n_heads: 12       # Proportionally increased
  n_encoder_layers: 8  # Deeper encoder
  n_decoder_layers: 8  # Deeper decoder
  d_ff: 3072        # Proportionally larger
  dropout: 0.2      # Increased regularization
  max_seq_length: 512
  
  # LoRA Configuration (only active during fine-tuning)
  use_lora: false   # Set to true only in fine-tuning phase
  lora:
    rank: 16        # LoRA rank
    alpha: 32       # LoRA scaling factor
    dropout: 0.05
    target_modules:
      - "query"
      - "value"
      - "key"
    # Percentage of parameters that are trainable with LoRA (rest frozen)
    trainable_ratio: 0.01  # ~1% of total parameters

# Training - Initial Phase (General Domain)
training:
  batch_size: 128   # Increased from 64
  epochs: 40        # Extended for more data
  
  # Optimizer
  optimizer: "adamw"
  learning_rate: 0.0005  # Lower for stability with larger model
  weight_decay: 0.01
  
  # Learning rate schedule
  scheduler: "cosine"  # Cosine annealing
  warmup_steps: 8000   # Longer warmup
  
  # Regularization
  label_smoothing: 0.1
  gradient_accumulation_steps: 1  # Effective batch = 128
  max_grad_norm: 1.0
  
  # Checkpointing & Evaluation
  save_every: 1000
  eval_every: 500
  log_every: 100
  
  # Logging
  use_wandb: true
  compute_bleu: true
  bleu_max_samples: 2000
  
  # Early stopping
  early_stopping_patience: 15

# LoRA Fine-tuning Configuration (2nd Phase)
lora_finetuning:
  enabled: true
  checkpoint: null  # Load from training phase
  
  # LoRA-specific training
  batch_size: 64    # Smaller for memory efficiency
  epochs: 20
  learning_rate: 0.001  # Higher LR for LoRA fine-tuning
  warmup_steps: 2000
  
  # Discriminative learning rates for different layers
  use_discriminative_lr: true
  layer_lr_scale:
    encoder_early: 0.5    # Earlier encoder layers learn slower
    encoder_late: 1.0
    decoder_early: 0.5
    decoder_late: 1.0
    lora: 1.5             # LoRA adapters learn faster
  
  # Stochastic Weight Averaging for better convergence
  use_swa: true
  swa_start_epoch: 15
  
  eval_every: 200
  save_every: 500
  early_stopping_patience: 8

# Inference Configuration
inference:
  beam_size: 10     # Increased from 5
  max_decode_length: 128
  length_penalty: 0.7
  coverage_penalty: 0.0  # Can prevent repetition
  
  # Ensemble decoding (if multiple checkpoints available)
  ensemble_models: []  # Paths to models to ensemble
  ensemble_weight: 0.5

# Data Augmentation
augmentation:
  # Back-translation settings
  use_backtranslation: true
  backtranslation_checkpoint: null  # Will use trained En→Vi model
  
  # Monolingual English data for back-translation
  monolingual_tgt: "data/raw_IWSLT'15_en-vi/monolingual_en.txt"  # English mono data
  
  # Tag synthetic data for potential weighting
  tag_backtranslation: true  # Add special token for synthetic pairs
  
  # Other augmentation
  dropout_augmentation: false  # Random word dropout
  swap_languages: false  # Simple augmentation (not recommended for low-res)

# Paths
paths:
  checkpoint_dir: "experiments/iwslt_v4_vi2en_lora/checkpoints"
  log_dir: "experiments/iwslt_v4_vi2en_lora/logs"
  vocab_dir: "data/vocab_iwslt_v4_vi2en"
  
  # For back-translation
  backtranslation_dir: "data/backtranslation"
  en2vi_checkpoint: "experiments/iwslt_v2_en2vi/checkpoints/best_model.pt"  # En→Vi model for back-translation

device: "mps"
seed: 42

# Version Info
version:
  name: "iwslt_v4_vi2en_lora"
  description: "Vi→En with larger model, LoRA fine-tuning, and back-translation"
  improvements:
    - "Larger model: d_model=768, n_layers=8"
    - "LoRA for parameter-efficient fine-tuning"
    - "Back-translation synthetic data (50% ratio)"
    - "Discriminative learning rates"
    - "Stochastic Weight Averaging (SWA)"
    - "Larger beam search (size=10)"
    - "Better data filtering"
    - "Longer warmup and training"
  
  # Training phases
  training_phases:
    phase1:
      name: "General Domain Pre-training"
      duration: "~40 epochs"
      config: "Full transformer training"
      checkpoint: "experiments/iwslt_v4_vi2en_lora/checkpoints/best_model.pt"
    
    phase2:
      name: "LoRA Fine-tuning with Back-translation"
      duration: "~20 epochs"
      config: "LoRA training with synthetic data"
      checkpoint: "experiments/iwslt_v4_vi2en_lora/checkpoints/lora_best_model.pt"
