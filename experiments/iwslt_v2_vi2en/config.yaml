# IWSLT Version 2: Vi→En Translation with Improved Training
# Same improved setup as v2 but for IWSLT dataset

# Data Configuration
data:
  src_lang: "vi"
  tgt_lang: "en"
  train_src: "data/raw_IWSLT'15_en-vi/train.vi.txt"
  train_tgt: "data/raw_IWSLT'15_en-vi/train.en.txt"
  # Validation files - Using tst2012 for validation
  val_src: "data/raw_IWSLT'15_en-vi/tst2012.vi.txt"
  val_tgt: "data/raw_IWSLT'15_en-vi/tst2012.en.txt"
  val_split: 0.1 # 10% of training data for validation if val files don't exist
  test_src: "data/raw_IWSLT'15_en-vi/tst2013.vi.txt"
  test_tgt: "data/raw_IWSLT'15_en-vi/tst2013.en.txt"
  max_seq_length: 128

# Vocabulary
vocab:
  tokenization: "bpe" # Use BPE tokenization
  src_vocab_size: 16000 # Reduced for better token learning
  tgt_vocab_size: 16000
  min_freq: 2

# Model - Same architecture
model:
  d_model: 256
  n_heads: 4
  n_encoder_layers: 4
  n_decoder_layers: 4
  d_ff: 1024
  dropout: 0.1
  max_seq_length: 512

# Training - Improved & Optimized (SOTA)
training:
  batch_size: 64
  epochs: 30
  optimizer: "adamw"
  learning_rate: 0.0007
  weight_decay: 0.01
  betas: [0.9, 0.98]
  scheduler: "cosine"
  use_mixed_precision: true
  warmup_steps: 6000
  label_smoothing: 0.1
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0

  save_every: 20000
  eval_every: 500
  log_every: 100

  use_wandb: true
  early_stopping_patience: 12 # Increased from 5 for more patience

  # BLEU computation during validation
  compute_bleu: true
  bleu_max_samples: 1000 # Number of samples to use for BLEU calculation

# Inference
inference:
  beam_size: 5 # Beam search
  max_decode_length: 128
  length_penalty: 0.6

# Paths
paths:
  checkpoint_dir: "experiments/iwslt_v2_vi2en/checkpoints"
  log_dir: "experiments/iwslt_v2_vi2en/logs"
  vocab_dir: "data/vocab_iwslt_v2_vi2en"

device: "cuda"
seed: 42

# Version info
version:
  name: "iwslt_v2_vi2en"
  description: "IWSLT Vi→En translation - SOTA configuration with advanced techniques"
  improvements:
    - "Xavier initialization (prevents gradient issues)"
    - "Mixed precision training (FP16 for faster training)"
    - "Label smoothing (0.1)"
    - "AdamW optimizer with weight decay"
    - "Cosine learning rate schedule"
    - "Beam search (size=5)"
    - "Gradient accumulation (effective batch=128)"
    - "Early stopping (patience=12)"
    - "BPE tokenization (16k vocab)"
    - "Trained on IWSLT dataset"
