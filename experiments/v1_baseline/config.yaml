# Version 1: Baseline Transformer
# Standard configuration without optimizations

# Data Configuration
data:
  src_lang: "vi"
  tgt_lang: "en"
  train_src: "data/raw/train.vi.txt"
  train_tgt: "data/raw/train.en.txt"
  # Validation files (if not exist, will auto-split from training)
  val_src: "data/raw/val.vi.txt"
  val_tgt: "data/raw/val.en.txt"
  val_split: 0.1  # 10% of training data for validation if val files don't exist
  test_src: "data/raw/public_test.vi.txt"
  test_tgt: "data/raw/public_test.en.txt"
  max_seq_length: 128

# Vocabulary
vocab:
  src_vocab_size: 32000
  tgt_vocab_size: 32000
  min_freq: 2

# Model - Standard Transformer Base
model:
  d_model: 512
  n_heads: 8
  n_encoder_layers: 6
  n_decoder_layers: 6
  d_ff: 2048
  dropout: 0.1
  max_seq_length: 512

# Training - Basic setup
training:
  batch_size: 32
  epochs: 50
  optimizer: "adam"
  learning_rate: 0.0001
  scheduler: "warmup"
  warmup_steps: 4000
  label_smoothing: 0.0  # No label smoothing
  
  save_every: 1000
  eval_every: 500
  log_every: 100

# Inference
inference:
  beam_size: 1  # Greedy search only
  max_decode_length: 128

# Paths
paths:
  checkpoint_dir: "experiments/v1_baseline/checkpoints"
  log_dir: "experiments/v1_baseline/logs"
  vocab_dir: "data/vocab"

device: "cuda"
seed: 42

# Weights & Biases
wandb:
  project: "nlp-transformer-mt"
  entity: null

# Version info
version:
  name: "v1_baseline"
  description: "Baseline Transformer with standard configuration"
  notes: "No optimizations - serves as baseline for comparison"
