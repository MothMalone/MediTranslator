# IWSLT Version 4: Vi→En with LoRA Fine-tuning - MPS OPTIMIZED
# Balanced for Mac M1/M2/M3 with 8GB+ unified memory
# ~75-80M parameters instead of 120M to avoid OOM

# Data Configuration
data:
  src_lang: "vi"
  tgt_lang: "en"
  
  # Primary training data
  train_src: "data/raw_IWSLT'15_en-vi/train.vi.txt"
  train_tgt: "data/raw_IWSLT'15_en-vi/train.en.txt"
  
  # Back-translation synthetic data (optional, reduces batch size if enabled)
  backtranslation_enabled: false  # Set to true after generating synthetic data
  backtranslation_src: "data/backtranslation/synthetic_train.vi.txt"
  backtranslation_tgt: "data/backtranslation/synthetic_train.en.txt"
  backtranslation_ratio: 0.3  # Lower ratio for MPS to save memory
  
  # Validation & Test
  val_src: "data/raw_IWSLT'15_en-vi/tst2012.vi.txt"
  val_tgt: "data/raw_IWSLT'15_en-vi/tst2012.en.txt"
  test_src: "data/raw_IWSLT'15_en-vi/tst2013.vi.txt"
  test_tgt: "data/raw_IWSLT'15_en-vi/tst2013.en.txt"
  
  max_seq_length: 128
  # Data filtering
  filter_by_length: true
  min_length: 3
  max_length_ratio: 1.5

# Vocabulary
vocab:
  tokenization: "bpe"
  src_vocab_size: 16000
  tgt_vocab_size: 16000
  min_freq: 2

# Model - Medium size for MPS (25% increase, not 50%)
model:
  d_model: 640      # 512 + 25% (vs 768 for CUDA)
  n_heads: 10       # Divisible by 640
  n_encoder_layers: 7  # One extra layer
  n_decoder_layers: 7
  d_ff: 2560        # 25% increase
  dropout: 0.2
  max_seq_length: 512
  
  # LoRA Configuration
  use_lora: false   # Set to true only in fine-tuning phase
  lora:
    rank: 12        # Slightly lower rank for MPS (16 → 12)
    alpha: 24
    dropout: 0.05
    target_modules:
      - "query"
      - "value"
    trainable_ratio: 0.01

# Training - Phase 1 (General Domain) - MPS Optimized
training:
  batch_size: 64    # Reduced from 128 for MPS memory
  epochs: 25
  
  # Optimizer
  optimizer: "adamw"
  learning_rate: 0.0005
  weight_decay: 0.01
  
  # Learning rate schedule
  scheduler: "cosine"
  warmup_steps: 6000  # Shorter warmup for faster training
  
  # Regularization
  label_smoothing: 0.1
  gradient_accumulation_steps: 2  # Effective batch = 128 (like CUDA version)
  max_grad_norm: 1.0
  
  # Checkpointing & Evaluation
  save_every: 1000
  eval_every: 500
  log_every: 100
  
  # Logging
  use_wandb: true
  compute_bleu: true
  bleu_max_samples: 2000
  
  # Early stopping
  early_stopping_patience: 15

# LoRA Fine-tuning Configuration (Phase 2)
lora_finetuning:
  enabled: true
  checkpoint: null
  
  batch_size: 32    # Small batch for MPS LoRA phase
  epochs: 20
  learning_rate: 0.001
  warmup_steps: 1500
  
  use_discriminative_lr: true
  layer_lr_scale:
    encoder_early: 0.5
    encoder_late: 1.0
    decoder_early: 0.5
    decoder_late: 1.0
    lora: 1.5
  
  use_swa: true
  swa_start_epoch: 15
  
  eval_every: 200
  save_every: 500
  early_stopping_patience: 8

# Inference Configuration
inference:
  beam_size: 8      # Smaller beam for MPS inference
  max_decode_length: 128
  length_penalty: 0.7
  coverage_penalty: 0.0

# Data Augmentation
augmentation:
  use_backtranslation: false  # Optional - adds memory pressure
  tag_backtranslation: true

# Paths
paths:
  checkpoint_dir: "experiments/iwslt_v4_vi2en_mps/checkpoints"
  log_dir: "experiments/iwslt_v4_vi2en_mps/logs"
  vocab_dir: "data/vocab_iwslt_v4_vi2en_mps"
  backtranslation_dir: "data/backtranslation"
  en2vi_checkpoint: "experiments/iwslt_v2_en2vi/checkpoints/best_model.pt"

device: "mps"
seed: 42

# MPS-Specific Settings
mps_config:
  enable_operations_fallback: true  # Fall back to CPU for unsupported ops
  memoryformat: "channels_last"  # Better cache locality

# Version Info
version:
  name: "iwslt_v4_vi2en_mps"
  description: "Vi→En with LoRA - MPS optimized for Mac (M1/M2/M3)"
  improvements:
    - "Medium model: d_model=640 (balanced for MPS)"
    - "7-layer encoder/decoder (one deeper than v2)"
    - "LoRA for efficient fine-tuning"
    - "Gradient accumulation to simulate larger batches"
    - "SWA for better convergence"
    - "Optimized batch sizes for 8GB unified memory"
  
  training_phases:
    phase1:
      name: "General Domain Pre-training"
      duration: "~3 hours on M2"
      memory: "~6-7GB unified memory"
      checkpoint: "experiments/iwslt_v4_vi2en_mps/checkpoints/best_model.pt"
    
    phase2:
      name: "LoRA Fine-tuning"
      duration: "~1 hour on M2"
      memory: "~4-5GB unified memory"
      checkpoint: "experiments/iwslt_v4_vi2en_mps/checkpoints/lora_best_model.pt"
  
  expected_results:
    bleu_phase1: "12-13"
    bleu_phase2: "13-15"
    improvement_over_v2: "30-50%"
