# Medical Fine-tuning with Vocabulary Expansion (En→Vi)
# Strategy: Expand vocab to include medical terms, then fine-tune

# Data Configuration
data:
  src_lang: "en"
  tgt_lang: "vi"
  train_src: "data/raw/train.en.txt"
  train_tgt: "data/raw/train.vi.txt"
  val_split: 0.01 # 1% = ~5000 samples from 500k
  test_src: "data/raw/public_test.en.txt" # ~3000 samples
  test_tgt: "data/raw/public_test.vi.txt"
  max_seq_length: 256

# Vocabulary - Use base vocab (or expanded if available)
vocab:
  tokenization: "bpe" # Same as base model
  src_vocab_size: 16000
  tgt_vocab_size: 16000
  min_freq: 2

# Vocabulary Expansion (optional)
vocab_expansion:
  enabled: true # Use same vocab as base model (BPE handles unknown words)
  max_new_words_src: 10000
  max_new_words_tgt: 10000
  min_freq: 2

# Model Architecture (inherited from v2)
model:
  d_model: 256
  n_heads: 4
  n_encoder_layers: 4
  n_decoder_layers: 4
  d_ff: 1024
  dropout: 0.1
  max_seq_length: 512
  use_xavier_init: true

# LoRA Configuration
lora:
  enabled: false # Disabled - using full fine-tuning instead
  rank: 8
  alpha: 16
  dropout: 0.0
  target_modules: ["W_Q", "W_V"] # Apply LoRA to Q,V projections (query, value)

# Fine-tuning Configuration
training:
  resume_from: "experiments/iwslt_v2_en2vi/checkpoints/best_model.pt" # Pretrained checkpoint
  batch_size: 64
  epochs: 10
  optimizer: "adamw"
  learning_rate: 0.00002 # 2e-5 (optimized for full fine-tuning)
  weight_decay: 0.01
  betas: [0.9, 0.999]
  scheduler: "cosine"
  warmup_steps: 2000
  max_grad_norm: 1.0

  # Advanced techniques
  use_discriminative_lr: true # Different LR per layer group
  discriminative_lr_groups:
    embeddings: 0.5 # 1e-5 for embeddings (careful with pretrained embeddings)
    encoder: 0.7 # 1.4e-5 for encoder
    decoder: 1.0 # 2e-5 for decoder (learns fastest)

  use_swa: true # Stochastic Weight Averaging for better generalization
  swa_start_epoch: 7
  swa_lr: 0.000005 # 5e-6 (lower SWA LR for stability)

  label_smoothing: 0.1
  gradient_accumulation_steps: 2
  use_mixed_precision: true

  eval_every: 500
  log_every: 100
  save_every: 10000 # Save checkpoint every 1000 steps for resuming
  early_stopping_patience: 10
  use_wandb: true # Enable wandb logging

# Paths
paths:
  checkpoint_dir: "experiments/medical_vocab_expanded_en2vi/checkpoints"
  log_dir: "experiments/medical_vocab_expanded_en2vi/logs"
  vocab_dir: "data/vocab_iwslt_v2_en2vi" # Base model vocab (fallback)
  expanded_vocab_dir: "data/vocab_medical_expanded_en2vi" # Expanded vocab (auto-used if vocab_expansion.enabled: true)

# Inference
inference:
  beam_size: 5
  max_decode_length: 256
  length_penalty: 0.6

device: "cuda"
seed: 42

# Weights & Biases
wandb:
  enabled: true
  project: "nlp-medical-mt"
  name: "medical_vocab_expand_en2vi"
  tags:
    - "medical"
    - "en2vi"
    - "vocab_expansion"
    - "fine-tuning"

# Version
version:
  name: "medical_vocab_expand_en2vi"
  description: "Fine-tune v2 En→Vi with vocabulary expansion for medical domain"
  strategy: "Expand vocab + LoRA + discriminative LR + SWA"
