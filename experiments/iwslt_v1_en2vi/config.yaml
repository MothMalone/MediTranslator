# IWSLT Version 1: Baseline Transformer En→Vi
# Standard configuration without optimizations

# Data Configuration
data:
  src_lang: "en"
  tgt_lang: "vi"
  train_src: "data/raw_IWSLT'15_en-vi/train.en.txt"
  train_tgt: "data/raw_IWSLT'15_en-vi/train.vi.txt"
  # Validation files - Using tst2012 for validation
  val_src: "data/raw_IWSLT'15_en-vi/tst2012.en.txt"
  val_tgt: "data/raw_IWSLT'15_en-vi/tst2012.vi.txt"
  val_split: 0.1 # 10% of training data for validation if val files don't exist
  test_src: "data/raw_IWSLT'15_en-vi/tst2013.en.txt"
  test_tgt: "data/raw_IWSLT'15_en-vi/tst2013.vi.txt"
  max_seq_length: 128

# Vocabulary - Basic word-level tokenization
vocab:
  tokenization: "word" # Word-level (no BPE)
  src_vocab_size: 32000
  tgt_vocab_size: 32000
  min_freq: 2

# Model - Standard Transformer Base
model:
  d_model: 128
  n_heads: 4
  n_encoder_layers: 4
  n_decoder_layers: 4
  d_ff: 512 # 4x d_model
  dropout: 0.1
  max_seq_length: 512
  use_xavier_init: false # No Xavier initialization (random init)

# Training - Baseline (NO optimizations)
training:
  batch_size: 32
  epochs: 30
  optimizer: "adam" # Basic Adam (not AdamW)
  learning_rate: 0.0001 # Conservative learning rate
  weight_decay: 0.01 # No weight decay
  scheduler: "warmup" # Simple warmup (not cosine)
  warmup_steps: 4000
  label_smoothing: 0.0 # No label smoothing
  gradient_accumulation_steps: 1 # No gradient accumulation
  max_grad_norm: 5.0 # Higher clipping threshold
  use_mixed_precision: false # No mixed precision (AMP)

  use_wandb: true
  save_every: 10000
  eval_every: 500
  log_every: 100

  # BLEU computation during validation
  compute_bleu: true
  bleu_max_samples: 1000 # Number of samples to use for BLEU calculation

# Inference
inference:
  beam_size: 1 # Greedy search only
  max_decode_length: 128

# Paths
paths:
  checkpoint_dir: "experiments/iwslt_v1_en2vi/checkpoints"
  log_dir: "experiments/iwslt_v1_en2vi/logs"
  vocab_dir: "data/vocab_iwslt_v1_en2vi"

device: "cuda"
seed: 42

# Weights & Biases
wandb:
  project: "nlp-transformer-mt"
  entity: null

# Version info
version:
  name: "iwslt_v1_en2vi"
  description: "IWSLT Baseline Transformer En→Vi - NO optimizations"
  baseline_features:
    - "Basic Adam optimizer (no AdamW)"
    - "No mixed precision training"
    - "No label smoothing"
    - "Simple warmup scheduler (no cosine)"
    - "Word-level tokenization (no BPE)"
    - "No gradient accumulation"
    - "Greedy decoding (no beam search)"
    - "Small model (d_model=128, 4 layers)"
  purpose: "Baseline model for comparison with optimized v2/v3"
